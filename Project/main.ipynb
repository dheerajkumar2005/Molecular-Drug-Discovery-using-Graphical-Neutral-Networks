{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pubchempy as pcp\n",
    "import sys\n",
    "import os\n",
    "import rdkit\n",
    "import sklearn\n",
    "import torch\n",
    "import torch_geometric\n",
    "import tqdm\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of molecules in Lipophilicity dataset: 4200\n",
      "Dataset attributes:\n",
      "force_reload: False\n",
      "has_download: True\n",
      "has_process: True\n",
      "log: True\n",
      "name: lipo\n",
      "names: {'esol': ('ESOL', 'delaney-processed.csv', 'delaney-processed', -1, -2), 'freesolv': ('FreeSolv', 'SAMPL.csv', 'SAMPL', 1, 2), 'lipo': ('Lipophilicity', 'Lipophilicity.csv', 'Lipophilicity', 2, 1), 'pcba': ('PCBA', 'pcba.csv.gz', 'pcba', -1, slice(0, 128, None)), 'muv': ('MUV', 'muv.csv.gz', 'muv', -1, slice(0, 17, None)), 'hiv': ('HIV', 'HIV.csv', 'HIV', 0, -1), 'bace': ('BACE', 'bace.csv', 'bace', 0, 2), 'bbbp': ('BBBP', 'BBBP.csv', 'BBBP', -1, -2), 'tox21': ('Tox21', 'tox21.csv.gz', 'tox21', -1, slice(0, 12, None)), 'toxcast': ('ToxCast', 'toxcast_data.csv.gz', 'toxcast_data', 0, slice(1, 618, None)), 'sider': ('SIDER', 'sider.csv.gz', 'sider', 0, slice(1, 28, None)), 'clintox': ('ClinTox', 'clintox.csv.gz', 'clintox', 0, slice(1, 3, None))}\n",
      "num_classes: 553\n",
      "num_edge_features: 3\n",
      "num_features: 9\n",
      "num_node_features: 9\n",
      "pre_filter: None\n",
      "pre_transform: None\n",
      "processed_dir: ./lipo/processed\n",
      "processed_file_names: data.pt\n",
      "processed_paths: ['./lipo/processed/data.pt']\n",
      "raw_dir: ./lipo/raw\n",
      "raw_file_names: Lipophilicity.csv\n",
      "raw_paths: ['./lipo/raw/Lipophilicity.csv']\n",
      "root: .\n",
      "slices: {'x': tensor([     0,     24,     57,  ..., 113526, 113547, 113568]), 'edge_index': tensor([     0,     54,    124,  ..., 247706, 247752, 247798]), 'edge_attr': tensor([     0,     54,    124,  ..., 247706, 247752, 247798]), 'smiles': tensor([   0,    1,    2,  ..., 4198, 4199, 4200]), 'y': tensor([   0,    1,    2,  ..., 4198, 4199, 4200])}\n",
      "transform: None\n",
      "url: https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/{}\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import MoleculeNet\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import pubchempy as pcp\n",
    "\n",
    "data = MoleculeNet(root=\".\", name=\"lipo\")\n",
    "print(f\"Number of molecules in Lipophilicity dataset: {len(data)}\")\n",
    "# Print dataset-level attributes (non-private, non-callable)\n",
    "print(\"Dataset attributes:\")\n",
    "for attr in dir(data):\n",
    "    if not attr.startswith(\"_\") and not callable(getattr(data, attr)):\n",
    "        print(f\"{attr}: {getattr(data, attr)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n",
    "# Split dataset into training, validation, and test sets\n",
    "torch.manual_seed(42)\n",
    "dataset = data.shuffle()\n",
    "\n",
    "# Ensure the node features are of type float\n",
    "dataset.x = dataset.x.float()\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "# test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset = dataset[:train_size]\n",
    "val_dataset = dataset[train_size:train_size + val_size]\n",
    "# test_dataset = dataset[train_size + val_size:]\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the GraphSAGE Model\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        \n",
    "        # Define six GraphSAGE layers\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv6 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        # Linear layer to align input dimensions for skip connections\n",
    "        self.project = torch.nn.Linear(in_channels, hidden_channels)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.fc2 = torch.nn.Linear(hidden_channels, out_channels)\n",
    "    def forward(self, data):\n",
    "        # Unpack the data object\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = x.to(torch.float32)\n",
    "\n",
    "        # Project input features to match hidden dimensions\n",
    "        x_proj = self.project(x)\n",
    "\n",
    "        # Apply SAGEConv layers with skip connections\n",
    "        x1 = F.relu(self.conv1(x, edge_index))\n",
    "        x2 = F.relu(self.conv2(x1 + x_proj, edge_index))  # Skip connection from input to second layer\n",
    "        x3 = F.relu(self.conv3(x2 + x1, edge_index))      # Skip connection from x1 to x3\n",
    "        x4 = F.relu(self.conv4(x3 + x2, edge_index))      # Skip connection from x2 to x4\n",
    "        x5 = F.relu(self.conv5(x4 + x3, edge_index))      # Skip connection from x3 to x5\n",
    "        x6 = F.relu(self.conv6(x5 + x4, edge_index))  \n",
    "        x = global_mean_pool(x6, batch)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = GraphSAGE(\n",
    "    in_channels=9,  # Input size matches node feature size\n",
    "    hidden_channels=128,  # Hidden layer size\n",
    "    out_channels=1  # Output size (1 for regression task)\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.9851, Train R²: 0.3284, Validation Loss: 0.9847, Validation R²: 0.2684\n",
      "Epoch: 002, Train Loss: 0.9484, Train R²: 0.3534, Validation Loss: 0.9935, Validation R²: 0.2625\n",
      "Epoch: 003, Train Loss: 0.9027, Train R²: 0.3845, Validation Loss: 0.9063, Validation R²: 0.3296\n",
      "Epoch: 004, Train Loss: 0.8606, Train R²: 0.4133, Validation Loss: 0.9065, Validation R²: 0.3307\n",
      "Epoch: 005, Train Loss: 0.8295, Train R²: 0.4345, Validation Loss: 0.8457, Validation R²: 0.3851\n",
      "Epoch: 006, Train Loss: 0.7904, Train R²: 0.4611, Validation Loss: 0.9330, Validation R²: 0.3299\n",
      "Epoch: 007, Train Loss: 0.7712, Train R²: 0.4742, Validation Loss: 0.7770, Validation R²: 0.4292\n",
      "Epoch: 008, Train Loss: 0.7540, Train R²: 0.4860, Validation Loss: 0.7674, Validation R²: 0.4372\n",
      "Epoch: 009, Train Loss: 0.7615, Train R²: 0.4808, Validation Loss: 0.8526, Validation R²: 0.3644\n",
      "Epoch: 010, Train Loss: 0.7253, Train R²: 0.5055, Validation Loss: 0.7330, Validation R²: 0.4606\n",
      "Epoch: 011, Train Loss: 0.6727, Train R²: 0.5414, Validation Loss: 0.7654, Validation R²: 0.4445\n",
      "Epoch: 012, Train Loss: 0.6567, Train R²: 0.5523, Validation Loss: 0.7219, Validation R²: 0.4658\n",
      "Epoch: 013, Train Loss: 0.6512, Train R²: 0.5560, Validation Loss: 0.6945, Validation R²: 0.4837\n",
      "Epoch: 014, Train Loss: 0.6144, Train R²: 0.5811, Validation Loss: 0.7242, Validation R²: 0.4698\n",
      "Epoch: 015, Train Loss: 0.5910, Train R²: 0.5971, Validation Loss: 0.6705, Validation R²: 0.5098\n",
      "Epoch: 016, Train Loss: 0.5892, Train R²: 0.5983, Validation Loss: 0.6485, Validation R²: 0.5178\n",
      "Epoch: 017, Train Loss: 0.5643, Train R²: 0.6153, Validation Loss: 0.6565, Validation R²: 0.5176\n",
      "Epoch: 018, Train Loss: 0.5856, Train R²: 0.6007, Validation Loss: 0.6569, Validation R²: 0.5144\n",
      "Epoch: 019, Train Loss: 0.5398, Train R²: 0.6320, Validation Loss: 0.6623, Validation R²: 0.5182\n",
      "Epoch: 020, Train Loss: 0.5848, Train R²: 0.6013, Validation Loss: 0.6199, Validation R²: 0.5443\n",
      "Epoch: 021, Train Loss: 0.5087, Train R²: 0.6531, Validation Loss: 0.6379, Validation R²: 0.5322\n",
      "Epoch: 022, Train Loss: 0.4869, Train R²: 0.6680, Validation Loss: 0.5722, Validation R²: 0.5771\n",
      "Epoch: 023, Train Loss: 0.4689, Train R²: 0.6803, Validation Loss: 0.5843, Validation R²: 0.5672\n",
      "Epoch: 024, Train Loss: 0.4757, Train R²: 0.6757, Validation Loss: 0.5695, Validation R²: 0.5773\n",
      "Epoch: 025, Train Loss: 0.4551, Train R²: 0.6897, Validation Loss: 0.6144, Validation R²: 0.5420\n",
      "Epoch: 026, Train Loss: 0.4574, Train R²: 0.6882, Validation Loss: 0.6113, Validation R²: 0.5485\n",
      "Epoch: 027, Train Loss: 0.4549, Train R²: 0.6899, Validation Loss: 0.5653, Validation R²: 0.5882\n",
      "Epoch: 028, Train Loss: 0.4290, Train R²: 0.7075, Validation Loss: 0.6322, Validation R²: 0.5383\n",
      "Epoch: 029, Train Loss: 0.4643, Train R²: 0.6834, Validation Loss: 0.5946, Validation R²: 0.5614\n",
      "Epoch: 030, Train Loss: 0.4430, Train R²: 0.6979, Validation Loss: 0.5715, Validation R²: 0.5793\n",
      "Epoch: 031, Train Loss: 0.4104, Train R²: 0.7202, Validation Loss: 0.5590, Validation R²: 0.5857\n",
      "Epoch: 032, Train Loss: 0.3797, Train R²: 0.7411, Validation Loss: 0.5937, Validation R²: 0.5718\n",
      "Epoch: 033, Train Loss: 0.3925, Train R²: 0.7324, Validation Loss: 0.8070, Validation R²: 0.4244\n",
      "Epoch: 034, Train Loss: 0.3706, Train R²: 0.7473, Validation Loss: 0.5373, Validation R²: 0.6095\n",
      "Epoch: 035, Train Loss: 0.3901, Train R²: 0.7340, Validation Loss: 0.5634, Validation R²: 0.5923\n",
      "Epoch: 036, Train Loss: 0.3654, Train R²: 0.7509, Validation Loss: 0.5290, Validation R²: 0.6151\n",
      "Epoch: 037, Train Loss: 0.3467, Train R²: 0.7636, Validation Loss: 0.4935, Validation R²: 0.6388\n",
      "Epoch: 038, Train Loss: 0.3574, Train R²: 0.7563, Validation Loss: 0.5218, Validation R²: 0.6166\n",
      "Epoch: 039, Train Loss: 0.3329, Train R²: 0.7730, Validation Loss: 0.5483, Validation R²: 0.6073\n",
      "Epoch: 040, Train Loss: 0.3272, Train R²: 0.7769, Validation Loss: 0.4946, Validation R²: 0.6403\n",
      "Epoch: 041, Train Loss: 0.3270, Train R²: 0.7771, Validation Loss: 0.4995, Validation R²: 0.6390\n",
      "Epoch: 042, Train Loss: 0.3015, Train R²: 0.7945, Validation Loss: 0.5171, Validation R²: 0.6230\n",
      "Epoch: 043, Train Loss: 0.3238, Train R²: 0.7792, Validation Loss: 0.4885, Validation R²: 0.6433\n",
      "Epoch: 044, Train Loss: 0.3299, Train R²: 0.7751, Validation Loss: 0.5268, Validation R²: 0.6178\n",
      "Epoch: 045, Train Loss: 0.3110, Train R²: 0.7879, Validation Loss: 0.5107, Validation R²: 0.6329\n",
      "Epoch: 046, Train Loss: 0.3019, Train R²: 0.7942, Validation Loss: 0.4965, Validation R²: 0.6344\n",
      "Epoch: 047, Train Loss: 0.2979, Train R²: 0.7969, Validation Loss: 0.5155, Validation R²: 0.6305\n",
      "Epoch: 048, Train Loss: 0.2852, Train R²: 0.8056, Validation Loss: 0.4692, Validation R²: 0.6629\n",
      "Epoch: 049, Train Loss: 0.2663, Train R²: 0.8184, Validation Loss: 0.4482, Validation R²: 0.6707\n",
      "Epoch: 050, Train Loss: 0.2966, Train R²: 0.7978, Validation Loss: 0.5526, Validation R²: 0.6111\n",
      "Epoch: 051, Train Loss: 0.2738, Train R²: 0.8134, Validation Loss: 0.4507, Validation R²: 0.6687\n",
      "Epoch: 052, Train Loss: 0.2601, Train R²: 0.8227, Validation Loss: 0.4930, Validation R²: 0.6410\n",
      "Epoch: 053, Train Loss: 0.2448, Train R²: 0.8331, Validation Loss: 0.4915, Validation R²: 0.6482\n",
      "Epoch: 054, Train Loss: 0.2566, Train R²: 0.8251, Validation Loss: 0.5037, Validation R²: 0.6300\n",
      "Epoch: 055, Train Loss: 0.2428, Train R²: 0.8345, Validation Loss: 0.4611, Validation R²: 0.6657\n",
      "Epoch: 056, Train Loss: 0.2405, Train R²: 0.8361, Validation Loss: 0.4611, Validation R²: 0.6632\n",
      "Epoch: 057, Train Loss: 0.2385, Train R²: 0.8374, Validation Loss: 0.4410, Validation R²: 0.6828\n",
      "Epoch: 058, Train Loss: 0.2472, Train R²: 0.8314, Validation Loss: 0.4754, Validation R²: 0.6511\n",
      "Epoch: 059, Train Loss: 0.2421, Train R²: 0.8350, Validation Loss: 0.4448, Validation R²: 0.6759\n",
      "Epoch: 060, Train Loss: 0.2259, Train R²: 0.8460, Validation Loss: 0.4782, Validation R²: 0.6597\n",
      "Epoch: 061, Train Loss: 0.2145, Train R²: 0.8538, Validation Loss: 0.4743, Validation R²: 0.6601\n",
      "Epoch: 062, Train Loss: 0.2368, Train R²: 0.8386, Validation Loss: 0.4752, Validation R²: 0.6547\n",
      "Epoch: 063, Train Loss: 0.2275, Train R²: 0.8449, Validation Loss: 0.4271, Validation R²: 0.6867\n",
      "Epoch: 064, Train Loss: 0.2090, Train R²: 0.8575, Validation Loss: 0.4304, Validation R²: 0.6811\n",
      "Epoch: 065, Train Loss: 0.2059, Train R²: 0.8596, Validation Loss: 0.4556, Validation R²: 0.6783\n",
      "Epoch: 066, Train Loss: 0.2102, Train R²: 0.8567, Validation Loss: 0.4784, Validation R²: 0.6533\n",
      "Epoch: 067, Train Loss: 0.1904, Train R²: 0.8702, Validation Loss: 0.4657, Validation R²: 0.6672\n",
      "Epoch: 068, Train Loss: 0.1927, Train R²: 0.8686, Validation Loss: 0.4255, Validation R²: 0.6897\n",
      "Epoch: 069, Train Loss: 0.1877, Train R²: 0.8720, Validation Loss: 0.4155, Validation R²: 0.7014\n",
      "Epoch: 070, Train Loss: 0.1900, Train R²: 0.8704, Validation Loss: 0.5130, Validation R²: 0.6258\n",
      "Epoch: 071, Train Loss: 0.1857, Train R²: 0.8734, Validation Loss: 0.4340, Validation R²: 0.6898\n",
      "Epoch: 072, Train Loss: 0.1846, Train R²: 0.8742, Validation Loss: 0.4681, Validation R²: 0.6661\n",
      "Epoch: 073, Train Loss: 0.1761, Train R²: 0.8799, Validation Loss: 0.4729, Validation R²: 0.6684\n",
      "Epoch: 074, Train Loss: 0.1615, Train R²: 0.8899, Validation Loss: 0.5128, Validation R²: 0.6404\n",
      "Epoch: 075, Train Loss: 0.1690, Train R²: 0.8848, Validation Loss: 0.4255, Validation R²: 0.6943\n",
      "Epoch: 076, Train Loss: 0.1719, Train R²: 0.8828, Validation Loss: 0.4320, Validation R²: 0.6909\n",
      "Epoch: 077, Train Loss: 0.1582, Train R²: 0.8921, Validation Loss: 0.4451, Validation R²: 0.6844\n",
      "Epoch: 078, Train Loss: 0.1810, Train R²: 0.8766, Validation Loss: 0.4092, Validation R²: 0.7051\n",
      "Epoch: 079, Train Loss: 0.1621, Train R²: 0.8895, Validation Loss: 0.4271, Validation R²: 0.6893\n",
      "Epoch: 080, Train Loss: 0.1541, Train R²: 0.8949, Validation Loss: 0.4462, Validation R²: 0.6775\n",
      "Epoch: 081, Train Loss: 0.1473, Train R²: 0.8996, Validation Loss: 0.4011, Validation R²: 0.7112\n",
      "Epoch: 082, Train Loss: 0.1497, Train R²: 0.8980, Validation Loss: 0.4582, Validation R²: 0.6649\n",
      "Epoch: 083, Train Loss: 0.1519, Train R²: 0.8964, Validation Loss: 0.4518, Validation R²: 0.6701\n",
      "Epoch: 084, Train Loss: 0.1512, Train R²: 0.8969, Validation Loss: 0.4239, Validation R²: 0.6926\n",
      "Epoch: 085, Train Loss: 0.1430, Train R²: 0.9025, Validation Loss: 0.4235, Validation R²: 0.6984\n",
      "Epoch: 086, Train Loss: 0.1489, Train R²: 0.8985, Validation Loss: 0.3998, Validation R²: 0.7122\n",
      "Epoch: 087, Train Loss: 0.1306, Train R²: 0.9110, Validation Loss: 0.4156, Validation R²: 0.6964\n",
      "Epoch: 088, Train Loss: 0.1461, Train R²: 0.9004, Validation Loss: 0.4268, Validation R²: 0.6943\n",
      "Epoch: 089, Train Loss: 0.1327, Train R²: 0.9095, Validation Loss: 0.4136, Validation R²: 0.7025\n",
      "Epoch: 090, Train Loss: 0.1267, Train R²: 0.9136, Validation Loss: 0.4283, Validation R²: 0.6931\n",
      "Epoch: 091, Train Loss: 0.1453, Train R²: 0.9009, Validation Loss: 0.4287, Validation R²: 0.6913\n",
      "Epoch: 092, Train Loss: 0.1280, Train R²: 0.9127, Validation Loss: 0.4292, Validation R²: 0.6939\n",
      "Epoch: 093, Train Loss: 0.1507, Train R²: 0.8973, Validation Loss: 0.4780, Validation R²: 0.6531\n",
      "Epoch: 094, Train Loss: 0.1308, Train R²: 0.9108, Validation Loss: 0.4539, Validation R²: 0.6790\n",
      "Epoch: 095, Train Loss: 0.1319, Train R²: 0.9101, Validation Loss: 0.4411, Validation R²: 0.6841\n",
      "Epoch: 096, Train Loss: 0.1222, Train R²: 0.9167, Validation Loss: 0.4274, Validation R²: 0.6958\n",
      "Epoch: 097, Train Loss: 0.1282, Train R²: 0.9126, Validation Loss: 0.4409, Validation R²: 0.6861\n",
      "Epoch: 098, Train Loss: 0.1147, Train R²: 0.9218, Validation Loss: 0.4296, Validation R²: 0.6938\n",
      "Epoch: 099, Train Loss: 0.1259, Train R²: 0.9142, Validation Loss: 0.4282, Validation R²: 0.6948\n",
      "Epoch: 100, Train Loss: 0.1121, Train R²: 0.9236, Validation Loss: 0.4302, Validation R²: 0.6932\n",
      "Epoch: 101, Train Loss: 0.1198, Train R²: 0.9183, Validation Loss: 0.4364, Validation R²: 0.6882\n",
      "Epoch: 102, Train Loss: 0.1165, Train R²: 0.9206, Validation Loss: 0.4851, Validation R²: 0.6586\n",
      "Epoch: 103, Train Loss: 0.1272, Train R²: 0.9133, Validation Loss: 0.4735, Validation R²: 0.6612\n",
      "Epoch: 104, Train Loss: 0.1120, Train R²: 0.9236, Validation Loss: 0.4516, Validation R²: 0.6846\n",
      "Epoch: 105, Train Loss: 0.1065, Train R²: 0.9274, Validation Loss: 0.4081, Validation R²: 0.7060\n",
      "Epoch: 106, Train Loss: 0.1111, Train R²: 0.9242, Validation Loss: 0.4728, Validation R²: 0.6653\n",
      "Epoch: 107, Train Loss: 0.1117, Train R²: 0.9239, Validation Loss: 0.4534, Validation R²: 0.6767\n",
      "Epoch: 108, Train Loss: 0.1123, Train R²: 0.9234, Validation Loss: 0.4485, Validation R²: 0.6792\n",
      "Epoch: 109, Train Loss: 0.1155, Train R²: 0.9212, Validation Loss: 0.4255, Validation R²: 0.6932\n",
      "Epoch: 110, Train Loss: 0.1058, Train R²: 0.9279, Validation Loss: 0.4329, Validation R²: 0.6881\n",
      "Epoch: 111, Train Loss: 0.1046, Train R²: 0.9287, Validation Loss: 0.4331, Validation R²: 0.6944\n",
      "Epoch: 112, Train Loss: 0.1012, Train R²: 0.9310, Validation Loss: 0.4453, Validation R²: 0.6856\n",
      "Epoch: 113, Train Loss: 0.0988, Train R²: 0.9326, Validation Loss: 0.4424, Validation R²: 0.6804\n",
      "Epoch: 114, Train Loss: 0.1000, Train R²: 0.9318, Validation Loss: 0.4257, Validation R²: 0.6942\n",
      "Epoch: 115, Train Loss: 0.0870, Train R²: 0.9407, Validation Loss: 0.4402, Validation R²: 0.6831\n",
      "Epoch: 116, Train Loss: 0.0956, Train R²: 0.9348, Validation Loss: 0.4403, Validation R²: 0.6881\n",
      "Epoch: 117, Train Loss: 0.0987, Train R²: 0.9327, Validation Loss: 0.4197, Validation R²: 0.6989\n",
      "Epoch: 118, Train Loss: 0.0938, Train R²: 0.9361, Validation Loss: 0.4335, Validation R²: 0.6898\n",
      "Epoch: 119, Train Loss: 0.0893, Train R²: 0.9391, Validation Loss: 0.4299, Validation R²: 0.6924\n",
      "Epoch: 120, Train Loss: 0.0917, Train R²: 0.9375, Validation Loss: 0.4020, Validation R²: 0.7142\n",
      "Epoch: 121, Train Loss: 0.0925, Train R²: 0.9370, Validation Loss: 0.4356, Validation R²: 0.6884\n",
      "Epoch: 122, Train Loss: 0.0856, Train R²: 0.9416, Validation Loss: 0.4517, Validation R²: 0.6748\n",
      "Epoch: 123, Train Loss: 0.0956, Train R²: 0.9348, Validation Loss: 0.4328, Validation R²: 0.6915\n",
      "Epoch: 124, Train Loss: 0.0984, Train R²: 0.9329, Validation Loss: 0.4449, Validation R²: 0.6824\n",
      "Epoch: 125, Train Loss: 0.0936, Train R²: 0.9362, Validation Loss: 0.5048, Validation R²: 0.6373\n",
      "Epoch: 126, Train Loss: 0.0838, Train R²: 0.9429, Validation Loss: 0.4637, Validation R²: 0.6757\n",
      "Epoch: 127, Train Loss: 0.0735, Train R²: 0.9499, Validation Loss: 0.4382, Validation R²: 0.6858\n",
      "Epoch: 128, Train Loss: 0.0723, Train R²: 0.9507, Validation Loss: 0.4414, Validation R²: 0.6792\n",
      "Epoch: 129, Train Loss: 0.0763, Train R²: 0.9480, Validation Loss: 0.4623, Validation R²: 0.6746\n",
      "Epoch: 130, Train Loss: 0.0800, Train R²: 0.9455, Validation Loss: 0.4308, Validation R²: 0.6938\n",
      "Epoch: 131, Train Loss: 0.0915, Train R²: 0.9376, Validation Loss: 0.4890, Validation R²: 0.6500\n",
      "Epoch: 132, Train Loss: 0.0825, Train R²: 0.9437, Validation Loss: 0.4154, Validation R²: 0.7018\n",
      "Epoch: 133, Train Loss: 0.0751, Train R²: 0.9488, Validation Loss: 0.4632, Validation R²: 0.6711\n",
      "Epoch: 134, Train Loss: 0.0679, Train R²: 0.9537, Validation Loss: 0.4336, Validation R²: 0.6902\n",
      "Epoch: 135, Train Loss: 0.0725, Train R²: 0.9506, Validation Loss: 0.4626, Validation R²: 0.6743\n",
      "Epoch: 136, Train Loss: 0.0715, Train R²: 0.9513, Validation Loss: 0.4214, Validation R²: 0.6957\n",
      "Epoch: 137, Train Loss: 0.0788, Train R²: 0.9463, Validation Loss: 0.4268, Validation R²: 0.6964\n",
      "Epoch: 138, Train Loss: 0.0742, Train R²: 0.9494, Validation Loss: 0.4596, Validation R²: 0.6772\n",
      "Epoch: 139, Train Loss: 0.0748, Train R²: 0.9490, Validation Loss: 0.4308, Validation R²: 0.6891\n",
      "Epoch: 140, Train Loss: 0.0722, Train R²: 0.9507, Validation Loss: 0.4190, Validation R²: 0.6980\n",
      "Epoch: 141, Train Loss: 0.0639, Train R²: 0.9564, Validation Loss: 0.4336, Validation R²: 0.6922\n",
      "Epoch: 142, Train Loss: 0.0669, Train R²: 0.9544, Validation Loss: 0.4334, Validation R²: 0.6942\n",
      "Epoch: 143, Train Loss: 0.0737, Train R²: 0.9497, Validation Loss: 0.4269, Validation R²: 0.6945\n",
      "Epoch: 144, Train Loss: 0.0762, Train R²: 0.9481, Validation Loss: 0.4485, Validation R²: 0.6757\n",
      "Epoch: 145, Train Loss: 0.0695, Train R²: 0.9526, Validation Loss: 0.4353, Validation R²: 0.6906\n",
      "Epoch: 146, Train Loss: 0.0684, Train R²: 0.9534, Validation Loss: 0.4379, Validation R²: 0.6841\n",
      "Epoch: 147, Train Loss: 0.0644, Train R²: 0.9561, Validation Loss: 0.4402, Validation R²: 0.6910\n",
      "Epoch: 148, Train Loss: 0.0785, Train R²: 0.9465, Validation Loss: 0.4492, Validation R²: 0.6764\n",
      "Epoch: 149, Train Loss: 0.0669, Train R²: 0.9544, Validation Loss: 0.4423, Validation R²: 0.6868\n",
      "Epoch: 150, Train Loss: 0.0615, Train R²: 0.9581, Validation Loss: 0.4314, Validation R²: 0.6950\n",
      "Epoch: 151, Train Loss: 0.0601, Train R²: 0.9590, Validation Loss: 0.4852, Validation R²: 0.6588\n",
      "Epoch: 152, Train Loss: 0.0609, Train R²: 0.9585, Validation Loss: 0.4620, Validation R²: 0.6683\n",
      "Epoch: 153, Train Loss: 0.0733, Train R²: 0.9500, Validation Loss: 0.4346, Validation R²: 0.6902\n",
      "Epoch: 154, Train Loss: 0.0578, Train R²: 0.9606, Validation Loss: 0.4334, Validation R²: 0.6931\n",
      "Epoch: 155, Train Loss: 0.0557, Train R²: 0.9620, Validation Loss: 0.4462, Validation R²: 0.6846\n",
      "Epoch: 156, Train Loss: 0.0626, Train R²: 0.9573, Validation Loss: 0.4428, Validation R²: 0.6857\n",
      "Epoch: 157, Train Loss: 0.0600, Train R²: 0.9591, Validation Loss: 0.4230, Validation R²: 0.6950\n",
      "Epoch: 158, Train Loss: 0.0677, Train R²: 0.9538, Validation Loss: 0.4376, Validation R²: 0.6882\n",
      "Epoch: 159, Train Loss: 0.0626, Train R²: 0.9573, Validation Loss: 0.4186, Validation R²: 0.6999\n",
      "Epoch: 160, Train Loss: 0.0635, Train R²: 0.9567, Validation Loss: 0.4214, Validation R²: 0.6935\n",
      "Epoch: 161, Train Loss: 0.0576, Train R²: 0.9607, Validation Loss: 0.4601, Validation R²: 0.6740\n",
      "Epoch: 162, Train Loss: 0.0647, Train R²: 0.9559, Validation Loss: 0.4377, Validation R²: 0.6899\n",
      "Epoch: 163, Train Loss: 0.0544, Train R²: 0.9629, Validation Loss: 0.4186, Validation R²: 0.7028\n",
      "Epoch: 164, Train Loss: 0.0541, Train R²: 0.9631, Validation Loss: 0.4750, Validation R²: 0.6605\n",
      "Epoch: 165, Train Loss: 0.0598, Train R²: 0.9592, Validation Loss: 0.4299, Validation R²: 0.6942\n",
      "Epoch: 166, Train Loss: 0.0496, Train R²: 0.9662, Validation Loss: 0.4164, Validation R²: 0.7032\n",
      "Epoch: 167, Train Loss: 0.0512, Train R²: 0.9651, Validation Loss: 0.4318, Validation R²: 0.6918\n",
      "Epoch: 168, Train Loss: 0.0550, Train R²: 0.9625, Validation Loss: 0.4190, Validation R²: 0.7011\n",
      "Epoch: 169, Train Loss: 0.0564, Train R²: 0.9616, Validation Loss: 0.4321, Validation R²: 0.6944\n",
      "Epoch: 170, Train Loss: 0.0588, Train R²: 0.9599, Validation Loss: 0.4751, Validation R²: 0.6652\n",
      "Epoch: 171, Train Loss: 0.0517, Train R²: 0.9648, Validation Loss: 0.4827, Validation R²: 0.6591\n",
      "Epoch: 172, Train Loss: 0.0526, Train R²: 0.9641, Validation Loss: 0.4728, Validation R²: 0.6722\n",
      "Epoch: 173, Train Loss: 0.0514, Train R²: 0.9649, Validation Loss: 0.4985, Validation R²: 0.6523\n",
      "Epoch: 174, Train Loss: 0.0549, Train R²: 0.9626, Validation Loss: 0.4341, Validation R²: 0.6914\n",
      "Epoch: 175, Train Loss: 0.0619, Train R²: 0.9578, Validation Loss: 0.4334, Validation R²: 0.6966\n",
      "Epoch: 176, Train Loss: 0.0500, Train R²: 0.9659, Validation Loss: 0.4483, Validation R²: 0.6776\n",
      "Epoch: 177, Train Loss: 0.0564, Train R²: 0.9615, Validation Loss: 0.4396, Validation R²: 0.6868\n",
      "Epoch: 178, Train Loss: 0.0473, Train R²: 0.9677, Validation Loss: 0.4785, Validation R²: 0.6602\n",
      "Epoch: 179, Train Loss: 0.0559, Train R²: 0.9619, Validation Loss: 0.4637, Validation R²: 0.6674\n",
      "Epoch: 180, Train Loss: 0.0567, Train R²: 0.9614, Validation Loss: 0.4323, Validation R²: 0.6888\n",
      "Epoch: 181, Train Loss: 0.0579, Train R²: 0.9605, Validation Loss: 0.4347, Validation R²: 0.6895\n",
      "Epoch: 182, Train Loss: 0.0489, Train R²: 0.9667, Validation Loss: 0.4284, Validation R²: 0.6896\n",
      "Epoch: 183, Train Loss: 0.0533, Train R²: 0.9637, Validation Loss: 0.4453, Validation R²: 0.6860\n",
      "Epoch: 184, Train Loss: 0.0497, Train R²: 0.9661, Validation Loss: 0.4179, Validation R²: 0.6997\n",
      "Epoch: 185, Train Loss: 0.0454, Train R²: 0.9691, Validation Loss: 0.4345, Validation R²: 0.6940\n",
      "Epoch: 186, Train Loss: 0.0498, Train R²: 0.9661, Validation Loss: 0.4463, Validation R²: 0.6864\n",
      "Epoch: 187, Train Loss: 0.0462, Train R²: 0.9685, Validation Loss: 0.4283, Validation R²: 0.6973\n",
      "Epoch: 188, Train Loss: 0.0435, Train R²: 0.9704, Validation Loss: 0.4664, Validation R²: 0.6707\n",
      "Epoch: 189, Train Loss: 0.0464, Train R²: 0.9683, Validation Loss: 0.4476, Validation R²: 0.6847\n",
      "Epoch: 190, Train Loss: 0.0438, Train R²: 0.9701, Validation Loss: 0.4416, Validation R²: 0.6843\n",
      "Epoch: 191, Train Loss: 0.0431, Train R²: 0.9706, Validation Loss: 0.4408, Validation R²: 0.6880\n",
      "Epoch: 192, Train Loss: 0.0507, Train R²: 0.9654, Validation Loss: 0.4469, Validation R²: 0.6796\n",
      "Epoch: 193, Train Loss: 0.0430, Train R²: 0.9707, Validation Loss: 0.4193, Validation R²: 0.7019\n",
      "Epoch: 194, Train Loss: 0.0471, Train R²: 0.9679, Validation Loss: 0.4438, Validation R²: 0.6853\n",
      "Epoch: 195, Train Loss: 0.0577, Train R²: 0.9607, Validation Loss: 0.4414, Validation R²: 0.6842\n",
      "Epoch: 196, Train Loss: 0.0564, Train R²: 0.9615, Validation Loss: 0.4441, Validation R²: 0.6876\n",
      "Epoch: 197, Train Loss: 0.0547, Train R²: 0.9627, Validation Loss: 0.4415, Validation R²: 0.6911\n",
      "Epoch: 198, Train Loss: 0.0450, Train R²: 0.9693, Validation Loss: 0.4505, Validation R²: 0.6808\n",
      "Epoch: 199, Train Loss: 0.0414, Train R²: 0.9718, Validation Loss: 0.4516, Validation R²: 0.6797\n",
      "Epoch: 200, Train Loss: 0.0419, Train R²: 0.9715, Validation Loss: 0.4363, Validation R²: 0.6875\n",
      "Test Loss: 0.4363, Test R²: 0.6875\n"
     ]
    }
   ],
   "source": [
    "# Define the Training Loop\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Collect true and predicted values for R² score calculation\n",
    "        all_true.append(data.y.cpu().numpy())\n",
    "        all_pred.append(out.cpu().detach().numpy())\n",
    "    \n",
    "    # Flatten the lists and compute R² score\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_pred = np.concatenate(all_pred)\n",
    "    r2 = r2_score(all_true, all_pred)\n",
    "    \n",
    "    return total_loss / len(loader), r2\n",
    "\n",
    "# Define the Validation and Testing Loop\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Collect true and predicted values for R² score calculation\n",
    "        all_true.append(data.y.cpu().numpy())\n",
    "        all_pred.append(out.cpu().detach().numpy())\n",
    "    \n",
    "    # Flatten the lists and compute R² score\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_pred = np.concatenate(all_pred)\n",
    "    r2 = r2_score(all_true, all_pred)\n",
    "    \n",
    "    return total_loss / len(loader), r2\n",
    "\n",
    "# Train the Model\n",
    "epochs = 200\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_r2 = train(model, train_loader)\n",
    "    val_loss, val_r2 = evaluate(model, val_loader)\n",
    "    print(f\"Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Train R²: {train_r2:.4f}, Validation Loss: {val_loss:.4f}, Validation R²: {val_r2:.4f}\")\n",
    "\n",
    "# Test the Model\n",
    "test_loss, test_r2 = evaluate(model, val_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test R²: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_env",
   "language": "python",
   "name": "general_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
